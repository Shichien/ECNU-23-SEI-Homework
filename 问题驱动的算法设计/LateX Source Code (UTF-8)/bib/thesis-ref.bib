@article{lemarechal2012cauchy,
  title={Cauchy and the gradient method},
  author={Lemar{\'e}chal, Claude},
  journal={Doc Math Extra},
  volume={251},
  number={254},
  pages={10},
  year={2012}
}

@online{coursera2024gradientdescent,
  title={What Is Gradient Descent in Machine Learning?},
  author={{Coursera Staff}},
  year={2024},
  note={Updated on Mar 21, 2024},
  url={https://www.coursera.org/articles/what-is-gradient-descent},
  journal={Coursera}
}

@article{lai2017solving,
  title={Solving nonlinear least squares problem using Gauss-Newton method},
  author={Lai, Wen Huey and Kek, Sie Long and Tay, Kim Gaik},
  journal={International Journal of Innovative Science, Engineering \& Technology},
  volume={4},
  number={1},
  pages={258--262},
  year={2017}
}

@article{hassan2009new,
  title={A new gradient method via quasi-Cauchy relation which guarantees descent},
  author={Hassan, Malik Abu and Leong, Wah June and Farid, Mahboubeh},
  journal={Journal of Computational and Applied Mathematics},
  volume={230},
  number={1},
  pages={300--305},
  year={2009},
  publisher={Elsevier}
}

@article{yang2020auto,
  title={Auto-ensemble: An adaptive learning rate scheduling based deep learning model ensembling},
  author={Yang, Jun and Wang, Fei},
  journal={IEEE Access},
  volume={8},
  pages={217499--217509},
  year={2020},
  publisher={IEEE}
}

@article{haji2021comparison,
  title={Comparison of optimization techniques based on gradient descent algorithm: A review},
  author={Haji, Saad Hikmat and Abdulazeez, Adnan Mohsin},
  journal={PalArch's Journal of Archaeology of Egypt/Egyptology},
  volume={18},
  number={4},
  pages={2715--2743},
  year={2021}
}

@inproceedings{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations},
  year={2015},
  address={San Diego, USA},
  note={Workshop Track},
  pages={1--13}
}

@article{zulaika2022students,
  title={Students’ Ability To Conduct Pre-Editing of Text Procedure for Google Neural Machine Translation},
  author={Zulaika, Baiq Almira and Baharuddin, Baharuddin and Wardana, Lalu Ali},
  journal={Journal of Language},
  volume={4},
  number={2},
  pages={173--183},
  year={2022}
}

@article{duchi2011adaptive,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2121--2159},
  year={2011}
}

@article{qian1999momentum,
  title={On the Momentum Term in Gradient Descent Learning Algorithms},
  author={Qian, Ning},
  journal={Neural Networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999}
}

@inproceedings{sutskever2013importance,
  title={On the Importance of Initialization and Momentum in Deep Learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  year={2013},
  address={Atlanta, USA},
  publisher={ACM},
  pages={III-1139--III-1147}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@article{schraudolph2002fast,
  title={Fast curvature matrix-vector products for second-order gradient descent},
  author={Schraudolph, Nicol N},
  journal={Neural computation},
  volume={14},
  number={7},
  pages={1723--1738},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{becker1988improving,
  title={Improving the convergence of back-propagation learning with second order methods},
  author={Becker, Sue and Le Cun, Yann and others},
  booktitle={Proceedings of the 1988 connectionist models summer school},
  pages={29--37},
  year={1988}
}

@article{nesterov2013gradient,
  title={Gradient Methods for Minimizing Composite Functions},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={140},
  number={1},
  pages={125--161},
  year={2013}
}

@inproceedings{bottou2010large,
  title={Large-scale Machine Learning with Stochastic Gradient Descent},
  author={Bottou, Léon},
  booktitle={Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT)},
  pages={177--186},
  year={2010},
  address={Paris, France},
  publisher={Physica-Verlag HD}
}

@misc{hinton2012neural,
  title={Neural Networks for Machine Learning: Lecture 6a Overview of Mini-batch Gradient Descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year={2012},
  howpublished={\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}},
  note={Online; accessed 2024-06-15}
}

@inproceedings{li2014efficient,
  title={Efficient Mini-batch Training for Stochastic Optimization},
  author={Li, Mu and Zhang, Tong and Chen, Yihong Q and Smola, Alexander J},
  booktitle={Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={661--670},
  year={2014},
  address={New York, USA},
  publisher={ACM}
}

@article{史加荣2021随机梯度下降算法研究进展,
  title={随机梯度下降算法研究进展},
  author={史加荣 and 王丹 and 尚凡华 and 张鹤于},
  journal={自动化学报},
  volume={47},
  number={9},
  pages={2103--2119},
  year={2021},
  publisher={自动化学报}
}

@inproceedings{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations},
  year={2015},
  address={San Diego, USA},
  note={Workshop Track},
  pages={1--13}
}

@article{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={In Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}

@misc{bilibili2024gradientdescent,
  title={Gradient Descent and Its Applications},
  author={Bilibili User},
  year={2024},
  note={Published on Bilibili, Accessed on 2024-06-15},
  url={https://www.bilibili.com/video/BV1YF411n7Dr/}
}

@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  booktitle={6th International Conference on Learning Representations (ICLR 2018)},
  year={2018},
  archivePrefix={arXiv},
  eprint={1904.09237}
}

@mastersthesis{rubio2017convergence,
  title={Convergence Analysis of an Adaptive Method of Gradient Descent},
  author={Rubio, David Mart{\'i}nez},
  year={2017},
  school={University of Oxford},
  type={Master's thesis},
  note={Retrieved 5 January 2024},
  url={https://example.com/path/to/thesis.pdf}
}

@misc{ruder2017optimizing,
  author = {Ruder, Sebastian},
  title = {An overview of gradient descent optimization algorithms},
  year = {2017},
  note = {Accessed: 2024-01-05},
  url = {https://www.ruder.io/optimizing-gradient-descent/#amsgrad}
}

@inproceedings{antonakopoulos2022adagrad,
  title={AdaGrad avoids saddle points},
  author={Antonakopoulos, Kimon and Mertikopoulos, Panayotis and Piliouras, Georgios and Wang, Xiao},
  booktitle={International Conference on Machine Learning},
  pages={731--771},
  year={2022},
  organization={PMLR}
}

@article{ward2020adagrad,
  title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={219},
  pages={1--30},
  year={2020}
}

@INPROCEEDINGS{8264077,
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={2017 IEEE 56th Annual Conference on Decision and Control (CDC)}, 
  title={Mini-batch gradient descent: Faster convergence under data sparsity}, 
  year={2017},
  volume={},
  number={},
  pages={2880-2887},
  keywords={Convergence;Optimization;Indexes;Support vector machines;Bipartite graph;Tools},
  doi={10.1109/CDC.2017.8264077}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}

@article{gupta2021artificial,
  title={Artificial intelligence to deep learning: machine intelligence approach for drug discovery},
  author={Gupta, Rohan and Srivastava, Devesh and Sahu, Mehar and Tiwari, Swati and Ambasta, Rashmi K and Kumar, Pravir},
  journal={Molecular diversity},
  volume={25},
  pages={1315--1360},
  year={2021},
  publisher={Springer}
}

@article{choudhuri2023recent,
  title={Recent advancements in computational drug design algorithms through machine learning and optimization},
  author={Choudhuri, Soham and Yendluri, Manas and Poddar, Sudip and Li, Aimin and Mallick, Koushik and Mallik, Saurav and Ghosh, Bhaswar},
  journal={Kinases and Phosphatases},
  volume={1},
  number={2},
  pages={117--140},
  year={2023},
  publisher={MDPI}
}